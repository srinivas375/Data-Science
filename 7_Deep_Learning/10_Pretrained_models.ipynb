{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UEZMPAG5m3e"
      },
      "source": [
        "Learning on Pretrained models\n",
        "* predictions using Pretrained models\n",
        "* Trasfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCryb__t5wmn"
      },
      "source": [
        "# **Pretrained Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbuFGwt15zQ6"
      },
      "source": [
        "* Pre-trained models are machine learning models that have been trained on massive datasets for general tasks (like image recognition or language understanding) and can be reused or fine-tuned for different, but related, tasks.\n",
        "* They provide a strong foundation, saving developers significant time, data, and computational resources compared to training models from scratch. This approach, known as transfer learning, allows practitioners to leverage existing expertise and achieve high performance with fewer resources, democratizing access to advanced AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wS_HYd6Hlh"
      },
      "source": [
        "Famous Pretrained models are\n",
        "* Xception\n",
        "* VGG\n",
        "* ResNet\n",
        "* Inception\n",
        "* MobileNet\n",
        "\n",
        "All the pretrained models of keras available in [Keras Pretrained Models](https://keras.io/api/applications/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dpidwCd62_S"
      },
      "source": [
        "## **Predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOOR1M7e6xB1"
      },
      "source": [
        "### **Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IAlpfWs538PX"
      },
      "outputs": [],
      "source": [
        "# loading the libraries\n",
        "\n",
        "import keras\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.xception import preprocess_input, decode_predictions\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyFj92uD7buk",
        "outputId": "91e4be29-b540-4d4d-9344-aab203663fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m91884032/91884032\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# loading the pretrined model\n",
        "\n",
        "xception_model = Xception(weights='imagenet') # imagenet dataset weights loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcKQ2PC673Cj"
      },
      "outputs": [],
      "source": [
        "img_path = '../datasets/elephant.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(299, 299))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcFMkwM_8Ni2",
        "outputId": "c2ad6f9b-b429-4a8d-c34f-841958e3fbf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ],
      "source": [
        "preds = xception_model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf-3cXPb8SGz",
        "outputId": "4a2e7575-e566-444b-9e76-884bbc87b257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Xception Predicted:\n",
            "African_elephant -> 0.6488386392593384\n",
            "tusker -> 0.23499467968940735\n",
            "Indian_elephant -> 0.017012067139148712\n"
          ]
        }
      ],
      "source": [
        "print('Xception Predicted:')\n",
        "for pred in decode_predictions(preds, top=3)[0]:\n",
        "  print(f\"{pred[1]} -> {pred[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FFbU809S6k"
      },
      "source": [
        "### **VGG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah9xx6DX8ZyB",
        "outputId": "0d0c2ecd-9de7-46b5-eaee-fe831cb99acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m574710816/574710816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.vgg19 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "vgg_model = VGG19(weights='imagenet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1AO9Svw-M0F",
        "outputId": "5192db2b-76b3-496c-cfc1-1cd40f1d1416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "VGG Predicted:\n",
            "water_ouzel -> 0.8631079196929932\n",
            "quail -> 0.08037760853767395\n",
            "black_grouse -> 0.030623333528637886\n"
          ]
        }
      ],
      "source": [
        "img_path = '../datasets/crow.jpg'  # crow images are not there in imagenet dataset\n",
        "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = vgg_model.predict(x)\n",
        "\n",
        "print('VGG Predicted:')\n",
        "for pred in decode_predictions(preds, top=3)[0]:\n",
        "  print(f\"{pred[1]} -> {pred[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmXczw0_aEE"
      },
      "source": [
        "### **ResNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqxZHm3r-fk4",
        "outputId": "ef57d6b2-d4a0-46b0-ba9e-57db407be59f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "resnet_model = ResNet50(weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWOZsHYX_sCe",
        "outputId": "dbab87bc-83a3-48a5-a46e-cc40c64bd9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "ResNet Predicted:\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "tusker -> 0.45289307832717896\n",
            "megalith -> 0.2751137912273407\n",
            "African_elephant -> 0.14027832448482513\n"
          ]
        }
      ],
      "source": [
        "img_path = '../datasets/elephant.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = resnet_model.predict(x)\n",
        "\n",
        "print('ResNet Predicted:')\n",
        "for pred in decode_predictions(preds, top=3)[0]:\n",
        "  print(f\"{pred[1]} -> {pred[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HontV_LpH2xo"
      },
      "source": [
        "### **Inception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UxulzTlyADnQ"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785JYjATIFli",
        "outputId": "d9f13170-4908-4623-c6b2-79b4a867d0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "inception_model = InceptionV3(weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_KPUjqKILtn",
        "outputId": "fe820296-6d29-4da4-ddb1-cbc448260c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "ResNet Predicted:\n",
            "African_elephant -> 0.49735528230667114\n",
            "tusker -> 0.21768619120121002\n",
            "Indian_elephant -> 0.052902814000844955\n"
          ]
        }
      ],
      "source": [
        "img_path = '../datasets/elephant.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(299, 299))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = inception_model.predict(x)\n",
        "\n",
        "print('ResNet Predicted:')\n",
        "for pred in decode_predictions(preds, top=3)[0]:\n",
        "  print(f\"{pred[1]} -> {pred[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaQ3BR5IW2K"
      },
      "source": [
        "### **MobileNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "348WC6Z4IQAQ"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.applications.mobilenet_v2 import decode_predictions, preprocess_input\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsmZXnvOIggF",
        "outputId": "08f0b3dd-85da-42fe-8daf-9a0e23f556a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "mobilenet_model = MobileNetV2(weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOnSYjHwI910",
        "outputId": "e607a926-bd7d-435f-dbe7-fbd0d5b2bad0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ac8f6418a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "ResNet Predicted:\n",
            "African_elephant -> 0.21883778274059296\n",
            "tusker -> 0.1936301589012146\n",
            "Indian_elephant -> 0.08512523770332336\n"
          ]
        }
      ],
      "source": [
        "img_path = '../datasets/elephant.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = mobilenet_model.predict(x)\n",
        "\n",
        "print('ResNet Predicted:')\n",
        "for pred in decode_predictions(preds, top=3)[0]:\n",
        "  print(f\"{pred[1]} -> {pred[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlN3h4abJMpj"
      },
      "source": [
        "## **Extracting Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rkx_KGUJRov"
      },
      "source": [
        "Instead of just predictions, we also extract the features of the given image, which is furthurly used for other tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9uTtZH0JCvI",
        "outputId": "588fbb2f-841b-4452-b1ff-4575901e1a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# extracting features with the help of VGG16\n",
        "\n",
        "import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=False) # removing the classifier part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3_6BS23J4kL",
        "outputId": "4e1f8258-026a-439e-9f40-01b0d8190c8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ac8ece7600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        }
      ],
      "source": [
        "img_path = '../datasets/elephant.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
        "x = keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "features = model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-e4hRUfKO1c",
        "outputId": "2e23915d-efe8-4d42-9a91-b51b652cdfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "shape of features: (1, 7, 7, 512)\n"
          ]
        }
      ],
      "source": [
        "print(type(features))\n",
        "print(f\"shape of features: {features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg3DaOfpKe0e"
      },
      "source": [
        "This feature part can be used for other tasks like recognition, identification and many other ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au4t39kIKeqd"
      },
      "source": [
        "# **Transfer Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4PNRlvK4uV"
      },
      "source": [
        "* Transfer learning is a machine learning technique where knowledge gained from solving one problem is reused to improve the performance on a related problem, especially when data for the new task is limited.\n",
        "*  It involves taking a pre-trained model—one already trained on a large dataset—and fine-tuning it for a new, similar task, which is particularly common and effective in domains like computer vision and natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bncM179gRxkt"
      },
      "source": [
        "* Feature Extraction\n",
        "* Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3VUUnGhR2Hf"
      },
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD2YGp8dKSDA"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp ../datasets/kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-w5VXdsFSwRI"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f57zRu5SEL9",
        "outputId": "65d924a4-197f-4b50-c492-0e1eb25977bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/salader/dogsvscats\n",
            "License(s): unknown\n",
            "Downloading dogsvscats.zip to /content\n",
            " 97% 1.04G/1.06G [00:05<00:00, 249MB/s]\n",
            "100% 1.06G/1.06G [00:05<00:00, 191MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d salader/dogsvscats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIJGbN5GSp6I"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('../datasets/dogsvscats.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gQ1c04w2TcQe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras.applications.vgg16 import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nJ3bfUrYDxk",
        "outputId": "5632fb99-724f-4afd-865c-eed5acb4119d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# loading datasets\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    'train/',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "val_ds = keras.utils.image_dataset_from_directory(\n",
        "    'test/',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7YJt79pJpQsq"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KuXPsKxztQx8"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDAIXAbUccx"
      },
      "source": [
        "![vgg16.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3JE717AeUUE_"
      },
      "outputs": [],
      "source": [
        "# feature extraction part\n",
        "conv_base = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rxVNnDxnVHcx"
      },
      "outputs": [],
      "source": [
        "# feezing the feature extraction part\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QML7Ia66Uw84"
      },
      "outputs": [],
      "source": [
        "# defining a custom classifier part\n",
        "\n",
        "model = keras.Sequential([\n",
        "    data_augmentation,\n",
        "    layers.Rescaling(1./255),\n",
        "    conv_base,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "iySxUjVnVQHb",
        "outputId": "5d9ad8ec-4c49-4e03-f3a2-e5d5c73b9183"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# observing the summary and trainable parameters\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bx3CEQGLsPZA"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYKJKGamtq2e",
        "outputId": "08bf5bd3-7386-4e39-ed74-3386f4f9bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 209ms/step - accuracy: 0.7986 - loss: 0.5361 - val_accuracy: 0.9108 - val_loss: 0.2052\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 209ms/step - accuracy: 0.8744 - loss: 0.2830 - val_accuracy: 0.9284 - val_loss: 0.1717\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 208ms/step - accuracy: 0.8872 - loss: 0.2634 - val_accuracy: 0.9162 - val_loss: 0.1852\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 177ms/step - accuracy: 0.8965 - loss: 0.2463 - val_accuracy: 0.9312 - val_loss: 0.1627\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 208ms/step - accuracy: 0.8906 - loss: 0.2429 - val_accuracy: 0.9234 - val_loss: 0.1753\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 208ms/step - accuracy: 0.8982 - loss: 0.2299 - val_accuracy: 0.9322 - val_loss: 0.1641\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 177ms/step - accuracy: 0.8986 - loss: 0.2353 - val_accuracy: 0.9218 - val_loss: 0.1825\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 208ms/step - accuracy: 0.9038 - loss: 0.2271 - val_accuracy: 0.9314 - val_loss: 0.1584\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 208ms/step - accuracy: 0.9021 - loss: 0.2250 - val_accuracy: 0.9360 - val_loss: 0.1559\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 208ms/step - accuracy: 0.9052 - loss: 0.2202 - val_accuracy: 0.9314 - val_loss: 0.1562\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtvu7mhHu3sM"
      },
      "source": [
        "We can also unfreeze some feature extraction layers.\n",
        "\n",
        "Then the training leads to 'fine tuning' the Pretrained model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
