{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ea9787a",
      "metadata": {
        "id": "3ea9787a"
      },
      "source": [
        "# **NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391f697c",
      "metadata": {
        "id": "391f697c"
      },
      "source": [
        "## What is NLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fce881a",
      "metadata": {
        "id": "9fce881a"
      },
      "source": [
        "**Natural Language Processing (NLP)** is a field of Artificial Intelligence that enables computers to understand, interpret, and generate human language.  \n",
        "It bridges the gap between human communication and computer understanding.\n",
        "\n",
        "NLP combines **linguistics**, **computer science**, and **machine learning** to analyze text and speech data.  \n",
        "Its applications are everywhere — from search engines and chatbots to sentiment analysis and language translation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e61bdc",
      "metadata": {
        "id": "55e61bdc"
      },
      "source": [
        "## Real-World Applications of NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3555288",
      "metadata": {
        "id": "e3555288"
      },
      "source": [
        "- **Machine Translation** — Google Translate, DeepL, etc.  \n",
        "- **Sentiment Analysis** — understanding emotions in reviews or tweets.  \n",
        "- **Chatbots & Virtual Assistants** — Siri, Alexa, ChatGPT, etc.  \n",
        "- **Information Retrieval** — search engines and document summarization.  \n",
        "- **Text Classification** — spam detection, topic categorization, etc.  \n",
        "- **Speech Recognition** — converting voice to text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a08db10f",
      "metadata": {
        "id": "a08db10f"
      },
      "source": [
        "## Core Tasks in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80eb4b46",
      "metadata": {
        "id": "80eb4b46"
      },
      "source": [
        "1. **Text Preprocessing**\n",
        "   - Tokenization  \n",
        "   - Stopword Removal  \n",
        "   - Stemming & Lemmatization  \n",
        "   - Part-of-Speech (POS) Tagging  \n",
        "\n",
        "2. **Text Respresentation**\n",
        "   - Bag of Words (BoW)  \n",
        "   - TF-IDF (Term Frequency–Inverse Document Frequency)  \n",
        "   - Word Embeddings (Word2Vec, GloVe, FastText)\n",
        "\n",
        "3. **Modeling**\n",
        "   - Classical ML Models (Naive Bayes, SVM, etc.)  \n",
        "   - Deep Learning Models (RNN, LSTM, Transformers, BERT)\n",
        "\n",
        "4. **Evaluation**\n",
        "   - Accuracy, Precision, Recall, F1-score  \n",
        "   - BLEU Score, ROUGE Score (for generation tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a69b64",
      "metadata": {
        "id": "d9a69b64"
      },
      "source": [
        "## NLP Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f557817",
      "metadata": {
        "id": "7f557817"
      },
      "source": [
        "1. **Data Collection** → Gather text data from various sources.  \n",
        "2. **Text Preprocessing** → Clean and structure the data.  \n",
        "3. **Feature Extraction** → Convert text into numerical features.  \n",
        "4. **Model Training** → Train a model to learn patterns from the text.  \n",
        "5. **Evaluation & Deployment** → Test and deploy the model for real use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da827f8f",
      "metadata": {
        "id": "da827f8f"
      },
      "source": [
        "## Popular NLP Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af427cf",
      "metadata": {
        "id": "5af427cf"
      },
      "source": [
        "| Library | Description |\n",
        "|----------|--------------|\n",
        "| **NLTK** | Great for beginners; includes basic NLP functions. |\n",
        "| **spaCy** | Industrial-strength NLP with fast tokenization and POS tagging. |\n",
        "| **scikit-learn** | Machine learning algorithms and text vectorization tools. |\n",
        "| **Transformers (Hugging Face)** | State-of-the-art pre-trained models like BERT, GPT, T5. |\n",
        "| **gensim** | Word embeddings and topic modeling. |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bd2c10",
      "metadata": {
        "id": "41bd2c10"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62243cd2",
      "metadata": {
        "id": "62243cd2"
      },
      "source": [
        "Natural Language Processing allows machines to:\n",
        "- Understand the meaning of human language.  \n",
        "- Process and analyze large amounts of text efficiently.  \n",
        "- Power intelligent systems that communicate naturally with users.\n",
        "\n",
        "> NLP is at the heart of modern AI — enabling language understanding, translation, summarization, and conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7af336ae",
      "metadata": {
        "id": "7af336ae"
      },
      "source": [
        "Installing Neccessay libraries: \n",
        "* Using pip:\n",
        "    - `pip install nltk spacy gensim scikit-learn transformers`\n",
        "* Using uv:\n",
        "     - `uv add nltk spacy gensim scikit-learn transformers`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e7ba08",
      "metadata": {
        "id": "52e7ba08"
      },
      "source": [
        "## **Key Terms**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e3548d",
      "metadata": {
        "id": "00e3548d"
      },
      "source": [
        "| Term | Meaning |\n",
        "|------|----------|\n",
        "| **Corpus** | A large, structured collection of texts or speech data used to train and test NLP models. |\n",
        "| **Document** | A single, self-contained unit of text within a corpus, such as a sentence, paragraph, email, or book.|\n",
        "| **Vocabulary** | The set of all unique words that are present across the entire corpus, typically without duplicates.|\n",
        "| **Word** | An individual token (sequence of characters) that serves as the basic element of meaning in the text.|\n",
        "| **Tokenization** | Breaking text into smaller units like words or sentences. |\n",
        "| **Stopwords** | Common words (like “the”, “is”, “and”) often removed during preprocessing. |\n",
        "| **Stemming** | Reducing a word to its base form (e.g., *running* → *run*). |\n",
        "| **Lemmatization** | Converting a word to its meaningful root using vocabulary and morphology (e.g., *better* → *good*). |\n",
        "| **POS Tagging** | Identifying parts of speech (noun, verb, adjective, etc.). |\n",
        "| **Named Entity Recognition (NER)** | Detecting entities like names, dates, or organizations. |\n",
        "| **Vectorization** | Converting text into numerical form for machine learning models. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c47fea04",
      "metadata": {
        "id": "c47fea04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Text Preprocessing tools\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Spacy for Adv NLP tasks\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876112b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "876112b1",
        "outputId": "d2be9a4a-4f30-4c23-bc1a-a1f268fa370d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# downloading necessay data\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EOrpOI2VqQO6",
      "metadata": {
        "id": "EOrpOI2VqQO6"
      },
      "source": [
        "## **Corpus, Document, vocabulory, words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bbdaca",
      "metadata": {
        "id": "36bbdaca"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Natural Language Processing is an exciting field of AI.\",\n",
        "    \"It allows machines to understand human language.\",\n",
        "    \"Tokenization, stemming, and lemmatization are common preprocessing steps of NLP.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c61fa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88c61fa9",
        "outputId": "cc89f7f9-a3a5-443e-a7a4-05f29dc7d460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Curpus: \n",
            "Natural Language Processing is an exciting field of AI.It allows machines to understand human language.Tokenization, stemming, and lemmatization are common preprocessing steps of NLP.\n"
          ]
        }
      ],
      "source": [
        "# corpus\n",
        "\n",
        "corpus_text = \"\"\n",
        "for text in corpus:\n",
        "    corpus_text += text\n",
        "\n",
        "print(f\"Curpus: \\n{corpus_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853a84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853a84ce",
        "outputId": "d47df10a-88c5-4a4d-ee7e-9ec6fcd86435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1: Natural Language Processing is an exciting field of AI.\n",
            "Document 2: It allows machines to understand human language.\n",
            "Document 3: Tokenization, stemming, and lemmatization are common preprocessing steps of NLP.\n"
          ]
        }
      ],
      "source": [
        "# Docuements\n",
        "\n",
        "for i, doc in enumerate(corpus):\n",
        "  print(f\"Document {i+1}: {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kQ8tZgzBrpRW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ8tZgzBrpRW",
        "outputId": "b796cd8c-f7f3-4333-d235-bd8bdb2d1309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'is', 'exciting', 'AI.', 'machines', 'common', 'lemmatization', 'preprocessing', 'language.', 'to', 'allows', 'Natural', 'are', 'Processing', 'NLP.', 'of', 'Language', 'Tokenization,', 'steps', 'field', 'stemming,', 'an', 'and', 'human', 'It', 'understand'}\n",
            "Vocabulary size: 25\n"
          ]
        }
      ],
      "source": [
        "# Vocabulory\n",
        "\n",
        "vocab = []\n",
        "for text in corpus:\n",
        "  vocab.extend(text.split(' '))\n",
        "print(f\"Vocabulary: {set(vocab)}\")\n",
        "print(f\"Vocabulary size: {len(set(vocab))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kf7xX3njsCGv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf7xX3njsCGv",
        "outputId": "2ecb75a0-212d-4919-8ebc-9b6b8cb217fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI.', 'It', 'allows', 'machines', 'to', 'understand', 'human', 'language.', 'Tokenization,', 'stemming,', 'and', 'lemmatization', 'are', 'common', 'preprocessing', 'steps', 'of', 'NLP.']\n",
            "Words size: 26\n"
          ]
        }
      ],
      "source": [
        "# words\n",
        "\n",
        "print(f\"Words: {vocab}\")\n",
        "print(f\"Words size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iFtMUiXwsvSv",
      "metadata": {
        "id": "iFtMUiXwsvSv"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bnu2AznVsz_X",
      "metadata": {
        "id": "bnu2AznVsz_X"
      },
      "source": [
        "**Tokenization** is the process of splitting text into smaller units called **tokens**.  \n",
        "These tokens can be **words**, **sentences**, or **subwords**, depending on the application.\n",
        "\n",
        "- **Word Tokenization:** Splitting text into words  \n",
        "- **Sentence Tokenization:** Splitting text into sentences  \n",
        "\n",
        "Tokenization is usually the **first step in text preprocessing** because most NLP tasks operate on tokens rather than raw text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TVT_2Do7tE3d",
      "metadata": {
        "id": "TVT_2Do7tE3d"
      },
      "source": [
        "Why Tokenization is Important\n",
        "\n",
        "1. Simplifies text processing  \n",
        "2. Helps in **counting words** and building **vocabulary**  \n",
        "3. Essential for **feature extraction** like Bag of Words or TF-IDF  \n",
        "4. Enables **stopword removal**, **stemming**, and **lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B6kLH6F9tDAj",
      "metadata": {
        "id": "B6kLH6F9tDAj"
      },
      "outputs": [],
      "source": [
        "text = \"Natural Language Processing enables machines to understand human language. It is fascinating!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpv7AYJgtP5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpv7AYJgtP5c",
        "outputId": "32908bcb-297c-44d3-b77f-72424e8cbcff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Tokenization:\n",
            "['Natural Language Processing enables machines to understand human language.', 'It is fascinating!']\n",
            "\n",
            "Number of sentences in text: 2\n"
          ]
        }
      ],
      "source": [
        "# sentence tokenization\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "print(\"\\nNumber of sentences in text:\", len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tY_k6e3ptSk1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY_k6e3ptSk1",
        "outputId": "2492eff7-42bd-4bdb-ef75-524c9c2adc68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.', 'It', 'is', 'fascinating', '!']\n",
            "Number of words in the corpus: 14\n"
          ]
        }
      ],
      "source": [
        "# word tokenization\n",
        "\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\")\n",
        "print(words)\n",
        "print(\"Number of words in the corpus:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C4Y0zraptsO5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4Y0zraptsO5",
        "outputId": "95cbe5f2-4fcb-4b04-feec-358db431e440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Tokens (lowercase, no punctuation):\n",
            "['natural', 'language', 'processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', 'it', 'is', 'fascinating']\n",
            "\n",
            "Number of filtered words: 12\n"
          ]
        }
      ],
      "source": [
        "# filtering the tokens\n",
        "\n",
        "words_filtered = [word.lower() for word in words if word.isalpha()]\n",
        "print(\"Filtered Tokens (lowercase, no punctuation):\")\n",
        "print(words_filtered)\n",
        "print(\"\\nNumber of filtered words:\", len(words_filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wjl0OQbauYmC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjl0OQbauYmC",
        "outputId": "5967ed38-3394-417e-8913-bd9500dab2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'machines', 'is', 'natural', 'human', 'enables', 'fascinating', 'language', 'to', 'understand', 'it', 'processing'}\n",
            "Vocabulary size: 11\n"
          ]
        }
      ],
      "source": [
        "# vocabulory from the filtered words\n",
        "\n",
        "print(f\"Vocabulary: {set(words_filtered)}\")\n",
        "print(f\"Vocabulary size: {len(set(words_filtered))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dK2hn6bevbMT",
      "metadata": {
        "id": "dK2hn6bevbMT"
      },
      "source": [
        "## **Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ttNn-0cwv2mD",
      "metadata": {
        "id": "ttNn-0cwv2mD"
      },
      "source": [
        "**Stemming** is the process of reducing words to their **root form** or **stem**.  \n",
        "It usually involves removing suffixes like `-ing`, `-ed`, `-s`.  \n",
        "\n",
        "- Example:  \n",
        "  - \"running\" → \"run\"  \n",
        "  - \"played\" → \"play\"  \n",
        "  - \"cats\" → \"cat\"\n",
        "\n",
        "Stemming is **rule-based** and may produce stems that are not real words.  \n",
        "It is often used in **search engines** or **text classification** where exact grammar is less important."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jo2a0C2jw5G-",
      "metadata": {
        "id": "jo2a0C2jw5G-"
      },
      "source": [
        "**Stemming Methods in NLP**\n",
        "\n",
        "**1. Porter Stemmer**  \n",
        "The **Porter Stemmer** is a rule-based stemming algorithm that removes common suffixes from words to reduce them to their stem form.  \n",
        "- Example: \"running\" → \"run\", \"played\" → \"play\"  \n",
        "- Pros: Simple and fast  \n",
        "- Cons: May produce non-dictionary stems\n",
        "\n",
        "**2. Snowball Stemmer**  \n",
        "The **Snowball Stemmer** (also called Porter2) is an improved version of the Porter Stemmer.  \n",
        "- Supports multiple languages  \n",
        "- More aggressive and consistent stemming than Porter  \n",
        "- Example: \"running\" → \"run\", \"easily\" → \"easili\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "krJBCI7iujTD",
      "metadata": {
        "id": "krJBCI7iujTD"
      },
      "outputs": [],
      "source": [
        "# imports for stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMCHwvxowFGr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMCHwvxowFGr",
        "outputId": "a1e41fa8-96b6-4334-826f-bb94b92ff32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial words: ['running', 'played', 'plays', 'easily', 'fairness']\n",
            "Porter Stemmer: ['run', 'play', 'play', 'easili', 'fair']\n"
          ]
        }
      ],
      "source": [
        "# applying the stemming with porterstemmer\n",
        "\n",
        "words = [\"running\", \"played\", \"plays\", \"easily\", \"fairness\"]\n",
        "\n",
        "porter = PorterStemmer()\n",
        "stemmed_porter = [porter.stem(word) for word in words]\n",
        "\n",
        "print(\"Initial words:\", words)\n",
        "print(\"Porter Stemmer:\", stemmed_porter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcivUGLbwQti",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcivUGLbwQti",
        "outputId": "b71e1510-e83c-4d89-b93c-ccdbc427a003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial words: ['running', 'played', 'plays', 'easily', 'fairness']\n",
            "Snowball Stemmer: ['run', 'play', 'play', 'easili', 'fair']\n"
          ]
        }
      ],
      "source": [
        "# applying the stemming with snowballstemmer\n",
        "\n",
        "words = [\"running\", \"played\", \"plays\", \"easily\", \"fairness\"]\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "stemmed_snowball = [snowball.stem(word) for word in words]\n",
        "\n",
        "print(\"Initial words:\", words)\n",
        "print(\"Snowball Stemmer:\", stemmed_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Ygv2rxXxOPO",
      "metadata": {
        "id": "1Ygv2rxXxOPO"
      },
      "source": [
        "## **Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w0hWQg9OxTuO",
      "metadata": {
        "id": "w0hWQg9OxTuO"
      },
      "source": [
        "**Lemmatization** reduces words to their **base or dictionary form** called a **lemma**.  \n",
        "Unlike stemming, it always returns a **real word**.  \n",
        "\n",
        "- Example:  \n",
        "  - \"running\" → \"run\"  \n",
        "  - \"better\" → \"good\"  \n",
        "  - \"cats\" → \"cat\"\n",
        "\n",
        "Lemmatization usually considers **part-of-speech (POS)** for more accurate results.  \n",
        "It is widely used in **NLP pipelines** where language understanding matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yzhBurMtwqCV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzhBurMtwqCV",
        "outputId": "350ebe8b-f13b-4b3a-badf-75047cafcfe3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import for lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r6weqyvAxjEh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6weqyvAxjEh",
        "outputId": "54aca9b9-6bfa-4c80-8774-802af00b72a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Words      Lemmatized\n",
            "----------------------------------------\n",
            "   running     running\n",
            "    played      played\n",
            "     plays        play\n",
            "    easily      easily\n",
            "    better      better\n",
            "      cats         cat\n"
          ]
        }
      ],
      "source": [
        "words = [\"running\", \"played\", \"plays\", \"easily\", \"better\", \"cats\"]\n",
        "\n",
        "# using wordnetLemmatizer (default POS='n')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(f\"{'Words':>10} {'Lemmatized':>15}\")\n",
        "print(\"-\"*40)\n",
        "for word, lemma in zip(words, lemmatized_words):\n",
        "    print(f\"{word:>10}  {lemma:>10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "w5zn-Llj8reD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5zn-Llj8reD",
        "outputId": "3c3afd70-1282-4167-b0f3-d3ba2e8c1116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Words      Lemmatized\n",
            "----------------------------------------\n",
            "   running         run\n",
            "    played        play\n",
            "     plays        play\n",
            "    easily      easily\n",
            "    better      better\n",
            "      cats         cat\n"
          ]
        }
      ],
      "source": [
        "# using wordnetLemmatizer for pos='v'\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(f\"{'Words':>10} {'Lemmatized':>15}\")\n",
        "print(\"-\"*40)\n",
        "for word, lemma in zip(words, lemmatized_words):\n",
        "    print(f\"{word:>10}  {lemma:>10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jqpYMTTP8kcU",
      "metadata": {
        "id": "jqpYMTTP8kcU"
      },
      "source": [
        "**Comparing the stemming and lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "PmTS8C91xy7p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmTS8C91xy7p",
        "outputId": "4545a3c6-920d-4e27-cbed-1e9e47776688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word       Porter     Snowball   Lemma (noun)    Lemma (verb)   \n",
            "running    run        run        running         run            \n",
            "played     play       play       played          play           \n",
            "cats       cat        cat        cat             cat            \n",
            "better     better     better     better          better         \n",
            "plays      play       play       play            play           \n",
            "eaten      eaten      eaten      eaten           eat            \n",
            "ate        ate        ate        ate             eat            \n"
          ]
        }
      ],
      "source": [
        "words = [\"running\", \"played\", \"cats\", \"better\", 'plays', 'eaten', 'ate']\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"{'Word':<10} {'Porter':<10} {'Snowball':<10} {'Lemma (noun)':<15} {'Lemma (verb)':<15}\")\n",
        "for word in words:\n",
        "    print(f\"{word:<10} {porter.stem(word):<10} {snowball.stem(word):<10} \"\n",
        "          f\"{lemmatizer.lemmatize(word):<15} {lemmatizer.lemmatize(word, pos='v'):<15}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VIPrH6ay980A",
      "metadata": {
        "id": "VIPrH6ay980A"
      },
      "source": [
        "## **StopWords**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIkEMPss-FZS",
      "metadata": {
        "id": "oIkEMPss-FZS"
      },
      "source": [
        "**Stopwords** are common words in a language that usually carry little meaningful information, such as:  \n",
        "`a, an, the, is, in, on, and, but`\n",
        "\n",
        "### **Why Remove Stopwords?**\n",
        "- Reduce noise in text data  \n",
        "- Improve efficiency of NLP models  \n",
        "- Focus on important keywords in tasks like classification or clustering\n",
        "\n",
        "### **Handling Stopwords in Python**\n",
        "- The **NLTK** library provides a ready-made list of stopwords  \n",
        "- Stopwords can be customized for specific tasks  \n",
        "- Example: Removing stopwords from a tokenized sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "cspyEoDo9iZc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cspyEoDo9iZc",
        "outputId": "7bb49584-3df3-4c31-b39c-a2cd8bc2bdff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing library and downloading stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "PWSHWpod_m7G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWSHWpod_m7G",
        "outputId": "88afcb3f-7ba4-4486-9688-e5b0cd46da7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tokens:\n",
            "['natural', 'language', 'processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', 'easily', '.', 'it', 'is', 'a', 'toolkit', 'for', 'handling', 'the', 'text']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Natural Language Processing allows machines to understand human language easily. It is a toolkit for handling the text\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(sentence.lower())\n",
        "print(\"Original Tokens:\")\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "FzSe6XlpGfCx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzSe6XlpGfCx",
        "outputId": "7dc2cd50-d568-44d7-df61-9647d42649d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Stopwords are:\n",
            "{'is', \"he's\", 'what', 'him', 'in', 'couldn', 'do', 'o', 'only', 'some', 'm', 'mightn', \"you'll\", 'he', 's', 'yourselves', 'won', 'from', \"you're\", 'them', 'were', 'shouldn', 'before', 'this', \"weren't\", 've', 'hers', \"wasn't\", 'yours', 'both', 'me', 'wouldn', \"we're\", 'hasn', \"they'll\", \"didn't\", \"i'd\", 'just', 'has', 'be', 'down', 'isn', 'at', 'but', 'they', 'those', 'if', 'then', 'itself', \"shan't\", \"it'll\", \"don't\", \"hadn't\", 'himself', \"that'll\", 'such', 'most', 'nor', 'under', 'out', 'whom', 'her', \"doesn't\", 'myself', 'as', 'than', 'we', \"haven't\", 'have', 'did', 'against', 'further', \"i've\", \"couldn't\", 'to', 'now', 'd', 'between', 'his', 'other', 'didn', \"mightn't\", 'themselves', 'shan', \"they're\", \"hasn't\", 'here', 'an', 'can', \"he'd\", 'theirs', 'who', 'll', 'which', 'being', 'their', 'she', 'aren', 'too', 'the', \"you've\", \"you'd\", 'how', 'few', 'while', 'above', 'about', 'or', 'that', 't', \"isn't\", 'again', 'very', \"i'll\", 'ain', 'not', 'same', \"he'll\", \"it's\", 'these', \"we've\", 'so', 'because', 'of', \"she'll\", \"she'd\", 'once', 'up', 'over', 'until', \"won't\", 'any', 'all', 'mustn', 'after', 'it', \"needn't\", 'with', 'don', \"it'd\", \"we'll\", 'doing', 'hadn', 'doesn', 'was', \"wouldn't\", 'does', \"should've\", 'there', 'off', \"aren't\", 'its', 'ourselves', \"i'm\", \"mustn't\", 'own', \"she's\", 'during', 'haven', \"they've\", 'y', \"shouldn't\", 'having', 're', 'needn', 'your', 'when', 'more', 'below', 'by', 'where', 'our', 'you', 'no', 'should', 'are', 'my', 'for', 'am', 'i', 'been', 'herself', 'ours', 'a', \"we'd\", 'had', 'wasn', 'ma', 'and', 'on', 'will', 'through', \"they'd\", 'each', 'why', 'into', 'yourself', 'weren'}\n",
            "Number of stopwords: 198\n"
          ]
        }
      ],
      "source": [
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"English Stopwords are:\")\n",
        "print(stop_words)\n",
        "print(f\"Number of stopwords: {len(stop_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "CETVLZk_GjHA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CETVLZk_GjHA",
        "outputId": "14b23d6a-75de-4000-8b63-258f7008e285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tokens:\n",
            "['natural', 'language', 'processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', 'easily', '.', 'it', 'is', 'a', 'toolkit', 'for', 'handling', 'the', 'text']\n",
            "\n",
            "Tokens after Stopwords Removal:\n",
            "['natural', 'language', 'processing', 'allows', 'machines', 'understand', 'human', 'language', 'easily', 'toolkit', 'handling', 'text']\n"
          ]
        }
      ],
      "source": [
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "print(\"Original Tokens:\")\n",
        "print(words)\n",
        "print(\"\\nTokens after Stopwords Removal:\")\n",
        "print(filtered_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "training_venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
