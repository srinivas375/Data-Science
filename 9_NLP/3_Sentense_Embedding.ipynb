{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4096e52",
   "metadata": {},
   "source": [
    "# **Sentence Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf596f70",
   "metadata": {},
   "source": [
    "**Sentence Embeddings** are vector representations of entire **sentences or documents**,  \n",
    "designed to capture their **semantic meaning** in a continuous vector space.\n",
    "\n",
    "While **Word Embeddings** represent individual words,  \n",
    "**Sentence Embeddings** summarize the **meaning of a whole sentence** as a single dense vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20f0d5",
   "metadata": {},
   "source": [
    "- Sentences with **similar meanings** → have **similar vectors**\n",
    "- Useful for:\n",
    "  - Semantic search  \n",
    "  - Sentence similarity and clustering  \n",
    "  - Text classification  \n",
    "  - Question answering and retrieval systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b4e47",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- Captures **sentence-level semantics**  \n",
    "- Enables comparison and clustering of text meaningfully  \n",
    "- Suitable for downstream NLP tasks (retrieval, QA, classification)\n",
    "\n",
    "**Limitations**\n",
    "- Some models are **computationally expensive**  \n",
    "- May still lose **fine-grained syntactic nuances**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31f848",
   "metadata": {},
   "source": [
    "## **AvgWord2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f66b7",
   "metadata": {},
   "source": [
    "**Average Word2Vec (AvgWord2Vec)** is a simple technique to create a **fixed-length vector representation for an entire sentence or document** by averaging the **Word2Vec embeddings** of all words in it.  \n",
    "Averaging the embeddings smooths out noise and gives a **semantic summary** of the sentence.  \n",
    "Similar sentences will have **similar average vectors**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07fd24",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- Simple and efficient  \n",
    "- Converts variable-length text into fixed-size vectors  \n",
    "- Works well as baseline features for classification or clustering\n",
    "\n",
    "**Limitations**\n",
    "- Ignores **word order and syntax**  \n",
    "- All words contribute **equally**, regardless of importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dab90",
   "metadata": {},
   "source": [
    "We will use \"Pretrained word2vec\" and generate the \"AvgWord2Vec\" from that for simple demonstration of working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97aa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f9cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ptpl-652\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ptpl-652\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ptpl-652\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK components if you haven't already\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9a0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample sentences\n",
    "sentences = [\n",
    "    \"The horse is running fast across the wide field.\",\n",
    "    \"Data science is a fantastic field for women and men.\",\n",
    "    \"The old man fed the cat a fish.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869c641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Word2Vec model (word2vec-google-news-300)...\n",
      "Model loaded successfully. Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "# Loading the Pre-trained Word2Vec Model\n",
    "\n",
    "print(\"Loading pre-trained Word2Vec model (word2vec-google-news-300)...\")\n",
    "\n",
    "try:\n",
    "    model: KeyedVectors = api.load('word2vec-google-news-300')\n",
    "    VECTOR_SIZE = model.vector_size\n",
    "    print(f\"Model loaded successfully. Vector dimension: {VECTOR_SIZE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have internet access and sufficient memory (model is ~3.4GB uncompressed).\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77613a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK tools\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a133e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = sentence.lower()\n",
    "    \n",
    "    # Remove punctuation/special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove Stopwords and Lemmatize\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            # Lemmatization (converting to base form)\n",
    "            processed_tokens.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "            \n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9344eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word2Vec\n",
    "\n",
    "def get_avg_word2vec_vector(words, w2v_model, vector_size):\n",
    "\n",
    "    # Removing the OOV words (Out Of Vocabulary)\n",
    "    word_vectors = [w2v_model[word] for word in words if word in w2v_model]\n",
    "    \n",
    "    # If no words in the sentence are in the vocabulary, return a zero vector\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Convert list of vectors to a NumPy array for efficient averaging\n",
    "    vectors = np.array(word_vectors)\n",
    "    \n",
    "    # average vector across all found word vectors (axis=0)\n",
    "    avg_vector = np.mean(vectors, axis=0)\n",
    "    \n",
    "    return avg_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa55617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence Vector Results ---\n",
      "\n",
      "Sentence: The horse is running fast across the wide field.\n",
      "Tokens Used: ['horse', 'run', 'fast', 'across', 'wide', 'field']\n",
      "Vector (First 5 dimensions of 300):\n",
      "[-0.0418218   0.01549276  0.04768372  0.02396647 -0.02583504]\n",
      "Vector Shape: (300,)\n",
      "\n",
      "Sentence: Data science is a fantastic field for women and men.\n",
      "Tokens Used: ['data', 'science', 'fantastic', 'field', 'women', 'men']\n",
      "Vector (First 5 dimensions of 300):\n",
      "[-0.12813313  0.06978353  0.0892334   0.00399272  0.10479736]\n",
      "Vector Shape: (300,)\n",
      "\n",
      "Sentence: The old man fed the cat a fish.\n",
      "Tokens Used: ['old', 'man', 'feed', 'cat', 'fish']\n",
      "Vector (First 5 dimensions of 300):\n",
      "[ 0.06906738  0.1616211  -0.05200195  0.05683594  0.00507812]\n",
      "Vector Shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sentence Vector Results ---\")\n",
    "for sentence in sentences:\n",
    "    # Preprocess\n",
    "    tokenized_words = preprocess_text(sentence)\n",
    "    \n",
    "    # Get the average vector\n",
    "    sentence_vector = get_avg_word2vec_vector(\n",
    "        tokenized_words, \n",
    "        model, \n",
    "        VECTOR_SIZE\n",
    "    )\n",
    "    \n",
    "    # Print the sentence and its vector (showing only first 5 dimensions)\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Tokens Used: {tokenized_words}\")\n",
    "    print(f\"Vector (First 5 dimensions of {VECTOR_SIZE}):\")\n",
    "    # NumPy array formatting to show a cleaner output\n",
    "    print(sentence_vector[:5]) \n",
    "    print(f\"Vector Shape: {sentence_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "612da902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample similarity observation between the texts\n",
    "\n",
    "texts = [\n",
    "    \"I love this product\",\n",
    "    \"I hate this product\",\n",
    "    \"This product is working excellently\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b4aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for sentence in texts:\n",
    "    # Preprocess\n",
    "    tokenized_words = preprocess_text(sentence)\n",
    "    \n",
    "    # Get the average vector\n",
    "    sentence_vector = get_avg_word2vec_vector(\n",
    "        tokenized_words, \n",
    "        model, \n",
    "        VECTOR_SIZE\n",
    "    )\n",
    "    vectors.append(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0de4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'text1' and 'text2'\n",
      "text 1: I love this product\n",
      "text 2: I hate this product\n",
      "similarity: 0.77934355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"Similarity between 'text1' and 'text2'\\ntext 1: {texts[0]}\\ntext 2: {texts[1]}\")\n",
    "print('similarity:',cosine_similarity(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12845c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'text1' and 'text3'\n",
      "text 1: I love this product\n",
      "text 2: This product is working excellently\n",
      "similarity: 0.5230589\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity between 'text1' and 'text3'\\ntext 1: {texts[0]}\\ntext 2: {texts[2]}\")\n",
    "print('similarity:',cosine_similarity(vectors[0].reshape(1, -1), vectors[2].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869da718",
   "metadata": {},
   "source": [
    "* since **AvgWord2Vec** giving equal importance to all the words, the results are not satisfying\n",
    "* But it is the starting point for the birth of excellent sentence embedding models\n",
    "* As like **AvgWord2Vec**, we can also perform **AvgGlove** and **AvgFasttext**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c127389",
   "metadata": {},
   "source": [
    "## **SBERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d83f1",
   "metadata": {},
   "source": [
    "**Sentence-BERT (SBERT)** is a modification of **BERT (Bidirectional Encoder Representations from Transformers)**  \n",
    "designed specifically to create **sentence-level embeddings** that capture **semantic similarity** efficiently.\n",
    "\n",
    "It was introduced by **Reimers and Gurevych (2019)** in the paper:  \n",
    "> “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469110d",
   "metadata": {},
   "source": [
    "While **BERT** produces powerful contextual embeddings,  \n",
    "it’s computationally expensive to compare two sentences because it requires a **forward pass for each pair**.\n",
    "\n",
    "SBERT solves this by:\n",
    "- Using a **Siamese network architecture**\n",
    "- Generating **fixed-length sentence embeddings**\n",
    "- Allowing **cosine similarity** or **Euclidean distance** to measure semantic similarity directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a32cb",
   "metadata": {},
   "source": [
    "**Popular Pre-trained SBERT Models**\n",
    "\n",
    "| Model Name | Description |\n",
    "|-------------|--------------|\n",
    "| `all-MiniLM-L6-v2` | Lightweight, fast, 384-dim embeddings |\n",
    "| `all-mpnet-base-v2` | High-performance general-purpose model |\n",
    "| `paraphrase-MiniLM-L6-v2` | Optimized for paraphrase detection |\n",
    "| `multi-qa-MiniLM-L6-cos-v1` | For question-answer semantic search |\n",
    "| `distiluse-base-multilingual-cased` | Multilingual version for 50+ languages |\n",
    "\n",
    "All models are available via the **`sentence-transformers`** library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43016bb9",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- Produces **high-quality sentence embeddings**\n",
    "- Enables **semantic similarity**, **search**, and **clustering**\n",
    "- Much **faster** than vanilla BERT for pairwise sentence comparison\n",
    "- Many **pre-trained models** for different domains and languages\n",
    "\n",
    "**Limitations**\n",
    "- Context window limited to 512 tokens  \n",
    "- Still **static per sentence** (not token-level contextual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abd440",
   "metadata": {},
   "source": [
    "Using \"all-MiniLM-L6-V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d14fcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a013e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SentenceTransformer model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# downloading the model\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded SentenceTransformer model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60109fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Texts ---\n",
      "Sentence 1: The weather is lovely today.\n",
      "Sentence 2: It's so sunny and beautiful outside!\n",
      "Sentence 3: I'm driving to the grocery store now.\n",
      "Sentence 4: A cat chases a mouse.\n"
     ]
    }
   ],
   "source": [
    "# sample sentences\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny and beautiful outside!\",\n",
    "    \"I'm driving to the grocery store now.\",\n",
    "    \"A cat chases a mouse.\"\n",
    "]\n",
    "\n",
    "print(\"--- Sample Texts ---\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "093dadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence Vectors (Embeddings) ---\n",
      "Shape of embeddings tensor: torch.Size([4, 384])\n",
      "Embedding for Sentence 1 (first 5 dimensions): [0.01919573 0.1200854  0.15959834 0.0670659  0.0500748 ]\n",
      "Embedding for Sentence 2 (first 5 dimensions): [0.01488302 0.0534854  0.09693496 0.05794089 0.05688087]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "print(\"--- Sentence Vectors (Embeddings) ---\")\n",
    "print(f\"Shape of embeddings tensor: {embeddings.shape}\")\n",
    "print(f\"Embedding for Sentence 1 (first 5 dimensions): {embeddings[0][:5].cpu().numpy()}\")\n",
    "print(f\"Embedding for Sentence 2 (first 5 dimensions): {embeddings[1][:5].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1be4a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Similarity Scores (Cosine Similarity Matrix) ---\n",
      "tensor([[ 1.0000,  0.7014,  0.1942, -0.0239],\n",
      "        [ 0.7014,  1.0000,  0.1868,  0.0015],\n",
      "        [ 0.1942,  0.1868,  1.0000,  0.0564],\n",
      "        [-0.0239,  0.0015,  0.0564,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# performing cosine similarity\n",
    "\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "print(\"\\n--- Similarity Scores (Cosine Similarity Matrix) ---\")\n",
    "print(cosine_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad6719",
   "metadata": {},
   "source": [
    "* We can observe that\n",
    "    - As expected -- text1 and text2 are more similary\n",
    "    - text3 is less similar with text1 and text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a81e549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (Sentence 1 vs. Sentence 2): 0.7014 (High)\n",
      "Similarity (Sentence 1 vs. Sentence 3): 0.1942 (Low)\n"
     ]
    }
   ],
   "source": [
    "sim_1_2 = cosine_scores[0, 1].item()\n",
    "sim_1_3 = cosine_scores[0, 2].item()\n",
    "\n",
    "print(f\"Similarity (Sentence 1 vs. Sentence 2): {sim_1_2:.4f} (High)\")\n",
    "print(f\"Similarity (Sentence 1 vs. Sentence 3): {sim_1_3:.4f} (Low)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a5aaf",
   "metadata": {},
   "source": [
    "**Simple Semantic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41d26517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Semantic Search for Query: 'How was the whether today ?' ---\n",
      "Score: 0.3728, Sentence: The weather is lovely today.\n",
      "Score: 0.1642, Sentence: It's so sunny and beautiful outside!\n",
      "Score: 0.1908, Sentence: I'm driving to the grocery store now.\n",
      "Score: -0.0323, Sentence: A cat chases a mouse.\n"
     ]
    }
   ],
   "source": [
    "# Performing simple Semantic Search\n",
    "\n",
    "query = input(\"Enter the Query: \")\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# computing similarity score\n",
    "query_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "\n",
    "print(f\"\\n--- Semantic Search for Query: '{query}' ---\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Score: {query_scores[i].item():.4f}, Sentence: {sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
